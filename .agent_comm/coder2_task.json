{
  "agent_id": "coder2",
  "task_id": "task_3",
  "files": [
    {
      "name": "embeddings.py",
      "purpose": "Word/sentence embeddings",
      "priority": "medium"
    },
    {
      "name": "language_model.py",
      "purpose": "Language model components",
      "priority": "medium"
    }
  ],
  "project_info": {
    "project_name": "enhanced_cs.CL_2508.21049v1_Re_Representation_in_Sentential_Relation_Extractio",
    "project_type": "nlp",
    "description": "Enhanced AI project based on cs.CL_2508.21049v1_Re-Representation-in-Sentential-Relation-Extractio with content analysis. Detected project type: nlp (confidence score: 6 matches).",
    "key_algorithms": [
      "Capsules",
      "Theoretical",
      "Pretraining",
      "Machine",
      "Embedding",
      "Curriculum",
      "Routing",
      "Main",
      "Vanilla",
      "Pre-Trained"
    ],
    "main_libraries": [
      "torch",
      "numpy",
      "pandas"
    ]
  },
  "paper_content": "PDF: cs.CL_2508.21049v1_Re-Representation-in-Sentential-Relation-Extractio.pdf\nChunk: 1/1\n==================================================\n\n--- Page 1 ---\nRe-Representation in Sentential Relation Extraction with Sequence\nRouting Algorithm\nRamazan Ali Bahrami and Ramin Yahyapour\nGeorg-August-Universit\u00e4t G\u00f6ttingen and GWDG , G\u00f6ttingen, Germany\n{ramazan.bahrami, ramin.yahyapour}@gwdg.de\nAbstract\nSentential relation extraction (RE) is an impor-\ntant task in natural language processing (NLP).\nIn this paper we propose to do sentential RE\nwith dynamic routing in capsules. We first show\nthat the proposed approach outperform state\nof the art on common sentential relation ex-\ntraction datasets Tacred, Tacredrev, Retacred,\nand Conll04. We then investigate potential\nreasons for its good performance on the men-\ntioned datasets, and yet low performance on\nanother similar, yet larger sentential RE dataset,\nWikidata. As such, we identify noise in Wiki-\ndata labels as one of the reasons that can hin-\nder performance. Additionally, we show as-\nsociativity of better performance with better\nre-representation, a term from neuroscience re-\nferred to change of representation in human\nbrain to improve the match at comparison time.\nAs example, in the given analogous terms\nKing:Queen::Man:Woman, at comparison time,\nand as a result of re-representation, the simi-\nlarity between related head terms (King,Man),\nand tail terms (Queen,Woman) increases. As\nsuch, our observation show that our proposed\nmodel can do re-representation better than the\nvanilla model compared with. To that end, be-\nside noise in the labels of the distantly super-\nvised RE datasets, we propose re-representation\nas a challenge in sentential RE1.\n1 Introduction\nSentential relation extraction is about inferring the\nrelation between two entities in a given sentence.\nVarious sentential relation extraction datasets are\nconstructed with distant supervision. Accordingly,\nit is assumed that if two entities are related in a\nknowledge base such as Wikidata, they are also\nrelated according to the sentence that contain them.\nAs such, datasets often not only provide sentences\nwith corresponding entities, but also additional de-\ntails such as description, aliases, and types of enti-\nties. Accordingly, to improve performance, various\n1https://github.com/bahramiramazan/re-representation\nFigure 1: The need for re-representation in sentential\nrelation extraction (Text from Wikipedia). At com-\nparison time, the similarity between related terms in-\ncreases. Note the entity types as manual label for re-\nrepresentation. Note also the proportional word analogy\nconstructed from the given example here, \"Alan Turing\":\n1912 :: \"C.F.GauSS\":1777\nworks introduce complicated models that account\nfor extra additional details. To that end, the results\nof incorporating additional details are not coher-\nent. As example, while in some studies entity type\nhas shown to improve performance (Bastos et al.,\n2021), in some others however, it has shown to de-\ngrade performance(Vashishth et al., 2018). As such,\nweather there are still room for improvement or if\nlow performance are due to noise and error in the\nlabels, is an open research question. In this paper,\nwe aim to offer a deeper understanding of the task\nthat can help in defining a better goal and objective\nfor methods that incorporate the additional details\ninto the sentential context using complex models.\nTo do so, we propose an intuitive model that out\nperform state of the art on most dataset, and then\ninvestigate the reason for its better performance.\nTo propose our approach, we build on works from\nneuroscience. To that end, inference such as sen-\ntential relation extraction in which a relation from\none context, is mapped to a relation in another con-\ntext is referred to as analogical reasoning (Gentner,\n1983). In analogies, such as proportional wordarXiv:2508.21049v1  [cs.CL]  28 Aug 2025\n\n--- Page 2 ---\nanalogies of the form King:Queen::Man:Woman,\n\u2019King\u2019 is related to \u2019Queen\u2019 as \u2019Man\u2019 is related to\n\u2019Woman\u2019, even though pairs (King,Man) or heads\nand (Queen,Woman) or tails are different. Similarly\nin sentential relation extraction, given that accord-\ning to sentences Si, fori\u2208 {1,2, ..., N}; the head\nentity, eh\niand tail entity, et\niare similarly related, we\nhave analogous terms of the form eh\ni:et\ni::eh\nj:et\nj.\nTo that end, studies in neuroscience suggest that,\ncomparison as the foundation of any analogical rea-\nsoning, changes the representation of objects being\ncompared (Gentner and Kurtz, 2006; Boteanu and\nChernova, 2015; Loewenstein et al., 2000; Gentner\nand Namy, 1999). Specifically (Holyoak, 2025)\nsuggest re-representation, according to which peo-\nple changes the representation of entities in order\nto improve the match at comparison time(Gentner\nand Kurtz, 2006). In other words, given that analo-\ngies are about partial similarity in different contexts\n(Hummel and Doumas, 2023; Gentner and Mark-\nman, 1997), to map a relation from one context to a\nrelation in another context, it is therefore needed to\ndiscard some information in both contexts (Saitta\nand Zucker, 2013) Figure 1. Accordingly, we found\ndynamic routing in Capsules network suitable for\nthe task. Capsules were introduced first by (Hinton\net al., 2011), and dynamic routing in capsules by\n(Sabour et al., 2017). They can be thought of neu-\nrons that output different features of processed en-\ntity. We test the proposed algorithm on relation ex-\ntraction datasets wikidata (Sorokin and Gurevych,\n2017), Tacred (Zhang et al., 2017), Tacredrev (Alt\net al., 2020), Retacred (Stoica et al., 2021), and\nConll04 (Roth and Yih, 2004; Eberts and Ulges,\n2019). Our observations are summarized as fol-\nlows:\n\u2022Our proposed approach improve state of the\nart scores on sentential relations extraction\ndatasets Tacred, Tacredrev, Retacred, and\nConll04.\n\u2022We estimate a significant error rate in labels of\nWikidata, the dataset on which various studies\ntry to improve model performance by incorpo-\nrating the additional details through complex\nmodels.\n\u2022We show empirical evidence of re-\nrepresentation and its associativity with\nbetter sentential RE performance in neural\nnetwork.2 Related Works\nThe use of extra additional details about entities\nsuch as entity type, aliases and description, and the\nway they are incorporated into the sentential con-\ntext is one of the main theme of related works. As\nsuch, beside studies that addresses noise in the RE\ndatasets, the other works deal with extra additional\ndetails and how to best incorporate them in the con-\ntext. To that end, (Riedel et al., 2010) show that the\nvanilla distant supervised method used for gener-\nating sentential RE datasets, result in noisy labels,\nand proposes an improved version of the vanilla\nmethod, reducing error rate by 30%. Addition-\nally, in studies related to variants of the common\nsentential RE dataset Tacred (Zhang et al., 2017),\nTacredrev (Alt et al., 2020), and Retacred (Stoica\net al., 2021); it is shown that after relabeling the\nnoisy examples in Tacred, models improve perfor-\nmance by 8.0% (Tacredrev) and 14.3% (Retacred)\nof F1 score. Moreover, state of art performance\nfor Tacred, and its variants, is proposed by (Zhou\nand Chen, 2022; Park and Kim, 2021). They show\nthat incorporating abstract label of entities( entity\ntypes) improve model performance. Furthermore,\n(Sorokin and Gurevych, 2017) introduces Wiki-\ndata, a much larger dataset for sentential relation\nextraction based on the knowledge base wikidata\n(Table 1). To that end, to improve performance\nby enriching sentential context, in addition to en-\ntity type, (Nadgeri et al., 2021; Bastos et al., 2021)\nconsider integrating other side information such\nas entity description, and aliases, through complex\nmodels such as graph neural network. Moreover,\nin (Vashishth et al., 2018), the use of entity types\nand relation alias information for improving perfor-\nmance is discussed.\n3 Problem Formulation\nWe formulate the sentential RE based on the funda-\nmental assumption that it is a type of analogical rea-\nsoning. To the end, in relation extraction datasets,\naccording to sentences Si, andSjwe can construct\nproportional word analogies of the form eh\ni:et\ni::\neh\nj:et\nj, as we have King:Queen::Man:Woman.\nAs such, a claim based on the studies from neu-\nroscience is that our ideal proposed model shall do\nre-representation(Gentner and Kurtz, 2006). More\ncommonly, given Xh\ni:Xt\ni::Yh\nj:Yt\nj, re-\nrepresentation can be viewed equivalent to a trans-\nformation Fsuch that according to some similarity\nmeasures \u03c8, when Xh\niis related to Xt\ni, asYh\njis\n\n--- Page 3 ---\nFigure 2: Credit assignment in dynamic routing (Heinsen, 2022). The output here has 2 dimensions only, one for\npositivity, and the other for negativity. As example, given some sequence of vectors of depth h (25 here), sequence\nnumber n (number of tokens in the respective sentence), and dimension d (1024 here), and some configuration for\nthe expected output (here depth=1, d=2, n=1), the dynamic routing algorithm works as credit assignment system.\nAs such, projections of every feature in the input has limited credits at their disposal, and assigns it to the features in\nthe output. Summing credits over all hidden states for positive feature, result in a value that is greater when the\nexample is positive, and smaller otherwise(Example sentences are taken from Retacred).\nrelated to Yt\nj(positive examples) , we have :\n\u03c8(F(Xh\ni), F(Yh\nj))\u22431\n\u03c8(F(Xt\ni), F(Yt\nj))\u22431\nand when Xh\niis not related to Xt\ni, asYh\njis related\ntoYt\nj(negative examples), we have:\n\u03c8(F(Xh\ni), F(Yh\nj))\u2243 \u22121\n\u03c8(F(Xt\ni), F(Yt\nj))\u2243 \u22121\nAs such, the similarity function \u03c8returns 1 when\nterms come from positive examples and -1 other-\nwise. One common example for \u03c8is cosine similar-\nity between two given vectors. It is to note that, as\nin sentential RE, the sentences containing entities\nexpress the relation between entities, it is there-\nfore needed that any change of representation be\nconditioned on the contextual sentence. As such,\nformally the task can be presented as the minimiza-\ntion problem below:\nmin\nt\u2208[s,o]{\u03c8(F(Xt\ni|Si), F(Xt\nj|Sj)) + (\u22121)p}N\ni,j=1\nHere N stands for the number of instances in\nthe dataset, p=1 when both entities come from the\nsame relation, or from positive examples, and p=0\nwhen entities come form negative examples. Addi-\ntionally, Xt\nistands for the embeddings of an entity\nor word, and Siis the embeddings for the contex-\ntual sentence. Moreover, sstands for subject or\nhead entity, and ofor object or tail entity.4 Proposed Method\nOur proposed model assumes an embedding model\n\u2126, and a transformation Ffor obtaining the re-\nrepresentations from the embeddings. Before giv-\ning a detailed description of our proposed method,\nwe characterize it as follows:\n1Given some Xi, as the tokens representing\nsentences, our transformation F obtain a single\nvector x1d\n(out)of some dimension d for joint\nrepresentation of sentence containing entities\neh\ni, andet\ni, that are related with some relation\nR=ri.\n2Instead of working with an explicit similarity\nfunction such as cosine similarity, our model\nis trained to maximize the following condi-\ntional probability:\nP\u03b8\u0010\nR=ri|F\u0000\n\u2126(Xi)\u0001\u0011\nWith \u03b8being the model parameters.\n3We show that maximizing the above condi-\ntional probability as we propose in this section\nwill encourage the explicit similarity as was\nexplained in Section 3.\nGiven that entities and sentences are sequence of\nvectors, our transformation Fcan be such, given\na sequence of vectors, it outputs a single vector\nx1d\n(out). To that end, our proposed method for Fis\ndynamic routing in capsules. Capsules were intro-\nduced first by (Hinton et al., 2011), and dynamic\n\n--- Page 4 ---\nFigure 3: Over all architecture of our proposed model. We use different heads to classify the relation. On top is the\nDecoder, the head that translate from the sentence to entities post-fixed with relation id. Below it, are heads that\nimplement routing as described in the paper (Heinsen, 2022). In that, H1( gray module in the middle) will identify\npositivity and negativity of examples. Under it is H3, the head that find the representation of the relation or sentence\nwith entities marked as is shown in text on top of the diagram. Above H1 is H2, the head that calculate the joint\nrepresentation of concerned entities. To the left is the pre-trained large language model(LLM), the backbone from\nwhich we obtain embeddings.\nrouting in capsules by (Sabour et al., 2017). Cap-\nsules can be thought of a group of neurons that\noutput a particular feature of the entity processed.\nAs such, it is similar to a fully connected network,\nexcept that instead of individual neurons, they are\ngrouped to present a particular feature. Addition-\nally, the contribution of a capsule with dynamic\nrouting in a layer to the layer above it, is deter-\nmined not only by weight matrix, but also by rout-\ning coefficients which depends on the input (Li).\nThe dynamic routing in capsules network is also\ncalled voting by agreement, as a capsule\u2019s vote\nis greater for capsules with which it agrees (Li).\nThe dynamic routing algorithm used in this work\n(Heinsen, 2022), instead of voting by agreement,\ndescribe itself as credit assignment system Figure\n2.\nWith that being said, to obtain the embed-\ndings of the given sequence of words or sen-\ntence, we use some pre-trained model \u2126such as\nbert_base_uncased (Devlin et al., 2018) or roberta-\nlarge (Liu et al., 2019):\nXhd\nn(inp)= \u2126(X)WithXbeing the tokenized sequence of words for\na sentence, and n(inp)being the number of tokens\nin our input sequence, dbeing the embedding di-\nmension, and h representing the number of hidden\nstates in the pre-trained model.\nTo generate an output x1d\n(out)as representa-\ntion for the given sentence, the generated multi-\ndimensional matrix Xhd\nn(inp)is feed into the se-\nquence routing algorithm (Heinsen, 2022). We\ncall the routing module as the routing head. As\nsuch, the routing head used, is configured to con-\nvert a sequence of vectors of depth h, number of\nsequence n, representing a sentence, term or entity\ninto a single vector of some dimension d. Addi-\ntionally, for experiment, we also create some of the\nrouting heads to do some specific predefined tasks,\nand evaluate if adding them to the main routing\nhead can be of help. To that end, our main routing\nhead is used for obtaining the representation of the\nsentence with some marking for the concerned en-\ntities as is shown in top of the Figure 3. Moreover,\nas in most datasets, a significant portions of the\ndata are negative, we create an special routing head\nwith two features, one representing positivity ( the\n\n--- Page 5 ---\nDataset Tacred Tacredrev Retacred Conll04 wikidata\nNo of Relations 41 41 40 5 353\nNo of Abstract Entities 23 23 23 4 13533\nTrain Size 68,124 68,124 58465 1283 372,059\nEval Size 22,631 22,631 19,584 - -\nTest Size 15,509 15,509 13,418 422 360,334\nNegative Size 79.5% 79.5% 79.5% - 29%\nTable 1: Statistics of datasets used in this work.\nrelation between concerned entities is among our\nrelation set) and another one representing negativ-\nity of examples. The working of the mentioned\nrouting head is depicted in the Figure 2. A detailed\ndescription of routing heads and the baseline De-\ncoder is depicted in the Figure 3. In practice, we\nexperiment and compare performance of a single\nrouting head, and all routing heads combined with\nthe Decoder, referred here as collection of experts.\nEach head is characterized as follows:\n1.H1: Obtains the representation for positivity\nor negativity. The output for this head has\nonly 2 dimension, one representing positivity\nand another negativity Figure 2.\n2.H2: This head learns the joint representation\nof head and tail entities.\n3.H3: Is used to obtain the representation for the\nsentence containing concerned entities. H3 is\nthe main routing head.\n4.Decoder: We use a transformer based decoder\nfor the baseline model, as is shown in the\nFigure 3. As in the example in the mentioned\nFigure, the decoder uses the last hidden state\nfor the sentence as memory, and entities post-\nfixed by the corresponding relation id as the\ntarget.\n4.1 Optimization\nGiven the organization of our data into head and tail\nentities, eh\niandet\ni, and the corresponding sentences\nSifori\u2208N, and relation r\u2208R, with R being the\nrelation set, and the embedding model \u2126, and the\ntransformation Fbased on dynamic routing, and\nan instance of data as follows:\nD={(eh\ni, et\ni, Si, ri)}N\ni=1\nwhere ri\u2208R and i \u2208 {1,2,3, . . . , N }Where N is the dataset size. The transformation\nFbased on dynamic routing, learns a representa-\ntionxd\ni, with some dimension d, such that the loss\nbelow is minimized:\nL(\u03b8) =\u2212NX\ni=1logP\u03b8\u0010\nR=ri|F\u0000\n\u2126(Xi)\u0001\u0011\nHere Xibeing the tokens for the sentence Si, and\nF is the routing head. In practice, by concatenat-\ning the outputs produced by different heads, we\nexperiment if combining heads, also referred to as\ncollection of experts Figure 3 may be of any help.\nAdditionally, if Decoder is among selected heads,\nits loss will be added to the classifiers loss as in the\nFigure 3.\n5 Experiments\n5.1 Datasets\nWe test our model on several sentential relation\nextraction datasets. Specifically we test the pro-\nposed model on wikidata (Sorokin and Gurevych,\n2017), Tacred (Zhang et al., 2017), Tacredrev (Alt\net al., 2020), Retacred (Stoica et al., 2021), and\nConll04 (Roth and Yih, 2004; Eberts and Ulges,\n2019). In all datasets, except Conll04, negative ex-\nample make a significant portion of the examples.\nTo that end, there are several factors to note about\nthe datasets.\n1.The ratio of positive and negative examples:\nConll04 has no negative record, while wiki-\ndata has 22/29% of example as negative, and\nall Tacred variants have 79.5% of example as\nnegative.\n2.Number of entity types or manual abstract\nlabel of entities: All Tacred variants has 23\nabstract labels for entities as and according\nto name entity recognition types in stanford\n\n--- Page 6 ---\nNER system (Zhang et al., 2017). Conll04 has\nonly 4 different types of entities, and Wikidata\nhas the highest number of abstract labels for\nentities 13,533.\n3.Number of relations: From number of rela-\ntion points of view, wikidata has 353 relation\ntypes, which is the largest among all datasets\nconsidered, while Conll04 has only 5 types of\nrelations.\n5.2 Different Configuration of the Sentence\nThe assumption for our proposed model is that,\nin order to do re-representation, dynamic routing\ncan do feature selection . As such, we study our\nproposed model with different settings or config-\nuration of the sentence. Accordingly, a given sen-\ntence can provide different level of details about\nthe entities and their relations. As example, the sen-\ntence \"<Mask> was getting married to <Mask>.\",\nwherein the two concerned entities are masked, pro-\nvide less details as when entities are not masked.\nSimilarly, when entities are replaced by entity type,\nthe level of details are less than the original sen-\ntence. This is important as in order to do re-\nrepresentation, models need to do abstraction, and\ndiscard some unnecessary details. As such, manual\nabstract label of entities or entity types can perhaps\nmake the job of RE models on some datasets easier.\nTo that end, the following sentence configurations\nare used with markings as is shown in the Figure 3:\n\u2022Abstract: We replace surface form of the en-\ntity with entity type (abstract label of the en-\ntity). example: Germany or France is replaced\nby entity type Country.\n\u2022Mask: We replace surface form of the entity\nwith the placeholder, \u2019MASK\u2019, in the sen-\ntence.\n\u2022Entities: We use only surface form of the en-\ntity as is.\n\u2022Mix: The entity type, or abstract label for\nentity and its surface form is used together\nwith some marking. Example: \"x was getting\nmarried to y.\" is transformed into : \" [e11] +\nperson * x [e12] was getting married to [e21]\n# person &y [e22].\"Config Model Retacred Conll04\nMixH3 92.2(80.1) 100.0(100.0)\nDecoder 49.3(21.0) 78.6(79.8)\nEntitiesH3 89.7(58.5) 84.1(84.7)\nDecoder 50.4(31.5) 42.1(41.8)\nMASKH3 81.7(54.2) 80.1(79.3)\nDecoder \u2013 \u2013\nAbstractH3 75.2(48.5) 82.2(80.3)\nDecoder 29.1(13.0) 61.8(63.7)\nTable 2: Comparative performance of the routing head\nH3, and transformer based Decoder on different configu-\nration of sentence or information granularity. Recorded\nscores inside parenthesis are F1 Macro, and F1 Micro\notherwise. The backbone model is roberta-large.\n5.3 Experiment One: Comparative\nPerformance on Different Information\nGranularity\nWe investigate performance achievable with our\nproposed model, and the transformer based De-\ncoder on each sentence configuration described\nabove. As each configuration of the sentence pro-\nvide different level of details about entities and\ntheir relation, we refer to different sentence con-\nfiguration as different information granularity. Ac-\ncordingly, the relation between two entities in a\nsentence can be mostly predicted in all sentence\nconfiguration considered here; However, the best\nresult by the proposed model is when the entity\ntype is added to contextual sentence. For Decoder\nhowever, the best result changes across datasets\nconsidered Table 2. As such, on Retacred, De-\ncoder\u2019s best score is when entity type is not added\nto the sentence( configuration \"Entities\"). However\non Conll04, it is the other way around. Moreover,\non Retraced, Decoder have relatively low scores,\nwhile on Conll04, our Decoder\u2019s score( 78.6 F1\nMicro) is above state of the art ( with state of the\nart being 76.5, Tables 5, and Table 2).\n5.4 Experiment Two: Entity Types as Manual\nLabel for Re-Representation\nGiven that entity types increase the similarity as\nis expected for re-representation (a depicted ex-\nample can be seen in the Figure 1), we can view\nentity types as manual label for re-representation.\nTo that end, we extract entities from the respec-\ntive sentences, and train the proposed model on\nthe extracted entities and entity types. This help\n\n--- Page 7 ---\nus study entities and entity types in isolation. In\ndoing so, we consider all sentence configurations\n(except Mask) as explained in the Section 5.2. As\nsuch, for Conll04, our proposed model exhibit\nsame performance with configuration Abstract and\nconfiguration Mix Table 4. As such, we can con-\nclude that, on some datasets, the manual label for\nre-representations or entity types (configuration\nAbstract), result in best performance. A possible\nexplanation would be: when the entity types or\nmanual label of re-representation can predict the\nrelation, or given a relation the entity types for\nhead and tail entities can be predicted, such as in\nConll04, entity types alone (config Abstract) result\nin peak performance and less complexity Table 4.\n5.5 Experiment Three: Performance on\nVarying Number of Entity Types\nDoes increasing the number of relation and entity\ntypes, or increased complexity for re-representation\neffect performance? To that end, we already ob-\nserved the relatively good performance by Decoder\non Conll04, the dataset with 4 entity types and 5\nrelations only Table 2. As such, we also evaluated\nthe Decoder and the proposed model H3, on the\nsmaller subset of Retacred, person-person, having\nonly 6 relations and 1 entity type only. Additionally,\nafter training on the full dataset, we recorded the\nperformance on the same subset, person-person*.\nAccordingly, Decoder\u2019s performance is better when\nnumber of relation and entity types are smaller Ta-\nble 3, as was noted for Conll04. As such, the ex-\nperiment support the notion that transformer based\nDecoder changes performance across dataset pre-\nsumably due to larger number of entity types, and\nrelations. Unlike the Decoder, the proposed model\nexhibit relatively high performance across datasets,\nwith different number of relation, and entity types.\nDataset Subset H3 Decoder\nFull 92.2(80.1) 49.3(21.0)\nPerson-Person 93.0(82.2) 72.6(60.4)\nPerson-Person* 89.7(78.3) 51.6(38.2)\nTable 3: Performance on varying number of relation and\nentity types (config mix).Values inside the parenthesis\nare F1 Macro, and F1 micro otherwise. Person-Person\nis the subset of Retacred having head and tail entity\ntypes as person only. It is the largest subset of Retacred\ncategorized by head-tail entity types. Full is the entire\ndataset. Person-Person* is performance on the same\nsubset, but by the model trained on the full Retacred.Metrics Retacred Conll04\nMix 71.7 100.0\nEntities 71.3 48.1\nAbstract 62.0 100.0\nTable 4: RE using entities extracted from the sentence,\nand with routing head H2. The backbone model here is\nroberta-large. The reported values are F1 micro.\n5.6 Experiment Four: Comparison with State\nof the Art\nTo compare with state of the art, we trained our\nproposed model on the mentioned datasets, and\ndocumented the result. The result is shown in the\nTable 5. Our observations show that our proposed\nmodel outperforms state of the art on 4 datasets. To\nthat end, our routing head H3, with roberta-large as\nthe backbone, keeps a relatively high performance\non all datasets. It outperform state of the art on\nall dataset, except Wikidata. In the the Section 6.2\nwe show that noise is the main reason for the low\nperformance on Wikidata. Moreover, despite the\nextra complexity that use of all heads, or expert\nheads, adds to our main model, we noticed little\nimprovement. We therefore did not evaluate the\nexpert head on Wikidata. Lastly, for our proposed\nmodel H3, the difference with bert-base-uncased\n(Devlin et al., 2018) and roberta-large (Liu et al.,\n2019) as the backbone is noticeable.\n6 Observations\n6.1 Re-Representation in Neural Network\nAs suggested initially, treating sentential RE as\nanalogy, requires some form of re-representation\nto improve the match. To check if neural-network\nalso does re-representation, using a subset from\nRetacred test set, we create positive and negative\nanalogous entities of the form eh\ni:et\ni::eh\nj:et\nj\n, for all i, j\u2208 {1,2, .., N}such that the corre-\nsponding sentence SiandSjexpresses the same\nrelation between the corresponding entities in pos-\nitive examples and different relation in negative\nexamples. In doing so, we obtain the embedding\nfor a given entity in the sentence, by feeding the\nsentence into the backbone model, and then slice\nthe entity from the sentence embedding. We then\ncalculate the cosine similarity, and pairwise eu-\nclidean distance between respective head, and tail\nentities in both positive and negative examples. We\ncalculate the mentioned values across hidden states\n\n--- Page 8 ---\n(a) As can be seen, before training, the similarity between head and tail terms in positive (Heads +, Tails +) and negative (Heads\n-, Tails -) examples are barely distinguishable. However, after training, the model based on dynamic routing, does a good job of\nmaking head/tail terms more similar in positive examples, and dissimilar in negative examples.\n(b) The distinction between positive and negative examples are barely distinguishable before training both for head terms\n(Heads +, and Heads -) and also for tail terms(Tails +, Tails -). However, after training, and that also specially for the model\nbased on dynamic routing (*_route +, *_route -) the increase in the distance between head/tail terms in positive examples, are\nfar less intense than in negative examples.\nFigure 4: X-axis represent different hidden layers of the pre-trained LLM. Y-axis represent categories for which\nrepresentation\u2019s similarity or distance was calculated. + represent positive analogous examples, and - represent\nnegative analogous examples respectively. Heads and Tails are the related head and tail terms in proportional\nanalogy. As example in king:queen::man:woman , (king, man) are head, whereas (queen, woman) are tail. We report\nthe result of calculations obtained on representation after training with routing heads H3(Heads/Tails_route +/-) ,\nand transformer based Decoder (Heads/Tails_decoder -/+). We also report the same before training (Heads/Tails\n+/-).\n\n--- Page 9 ---\nModel Tacred Tacredrev Retacred Conll04 Wikidata\nEntity Marker (2022) 74.6 83.2 91.1 - -\nCurriculum Learning(2021) 75.2 - 91.4 - -\nREBEL (2021) - \u2013 90.4 76.5 -\nKGpool (2021) - - - - 88.6\nRAG4RE (2024) 86.6 88.3 73.3 - -\nOurs bert H3 84.8 (47.8) 85.3 (49.7) 89.4 (74.0) 99.7(99.8) 84.5 (32.0)\nOurs bert H1,H2,H3,Decoder 87.4 (48.3) 88.7(50.9) 88.7 (68.5) 100.(100.) \u2013\nOurs Roberta H3 87.1 (61.1) 88.8 (64.2) 92.2(80.1) 100. (100.) 85.6 (32.9)\nTable 5: Our method\u2019s performance compared with state of the art. Best score is bold, state of the art is blue. Values\ninside the parenthesis are F1 Macro, and F1 micro otherwise. The configuration of sentence is Mix, and backbone is\nas indicated. We do not test all heads(H1,H2,H3,Decoder) for Wikidata as we found H3\u2019s performance to be already\ngood on Wikidata\u2019s noisy labels.\nof pre-trained backbone model, both after train-\ning with each training heads H3 and Decoder, and\nalso before training, and then create a heat map\nas is shown in the Figure 4a. Accordingly, \"Head\n+\" , and \"Head -\" represent the cosine similarity\nbetween heads in positive and negative examples\nbefore training. As can be seen, the similarity is\nnot much different between positive and negative\nexamples. However, for the proposed model, after\ntraining, the similarity decreases significantly in\nnegative examples, making the difference between\npositive and negative examples clearly noticeable\n( specially in final layers of the backbone model).\nSimilarly, the pairwise euclidean distance between\npositive and negative examples, shown in Figure\n4b, after training are clearly distinguishable for\nthe proposed model(Heads_route +, Head_route -)\nthan it is for the vanilla Decoder (Heads_decoder\n+, Head_decoder -).\n6.2 Noise in Wikidata\u2019s Labels\nThe tow Tacred variants (Tacredrev, Retacred) are\nvery good attempts to improve data quality and re-\nduce error rate in the Tacred. Each of these datasets\nimprove model performance with 8.0% and 14.3%\nF1 score over the original Tacred respectively. In\ncomparison to Tacred, wikidata has much larger\nand diverse types of relations. Its quality however\nhas not gone a similar study. Instead, a significant\nattention has been given in improving model per-\nformance by incorporating extra additional details\nthrough complex models. As our model\u2019s perfor-\nmance is below state of the art on Wikidata, we\nwere intrigued to have a look at examples in which\nour model disagree with labels from the dataset.Not surprisingly though, we found out that a signif-\nicant portion of errors are due to confusion in the\ndataset labels. As example, for instances which our\nmodel disagree with the dataset label, the labels\nseem random. More such examples, and statistics\nin Appendix B.2. We categorize all examples that\nour model disagree with labels from dataset in the\nappendix B.2, Table 6 , Table 7, and Table 8.\n7 Limitations\nOver all the dynamic routing proposed by (Heinsen,\n2022) is efficient and scalable as is explained in\nthe original paper. However using all routing heads\nas collection of experts increases the complexity n\nfolds, where n is the number of routing heads in the\nmodel. However, the good news is that, perhaps a\nsingle H3 head can do a better job as is shown in\nthe Table 5.\n8 Conclusion and Future Research\nDirections\nIn this paper we improve sentential relation ex-\ntraction performance on several benchmarks. Ad-\nditionally we identify noise as one of the main\ncause for low performance on largest sentential RE\nbenchmark Wikidata. Furthermore, we propose\nre-representation as one of the challenges of sen-\ntential RE models. Lastly, we show that sentential\nRE dataset may not be as much sentence dependent\nas expected B.1. For future research direction, we\nare planing to study word analogies of the form\na:b::c:d, jointly with sentential RE datasets. Specif-\nically, it would be interesting to see how much im-\nprovement can training sentential RE benchmarks\nbring to word analogy benchmarks.\n\n--- Page 10 ---\nEthics Statement\nWe did pay for the dataset Tacred. Other datasets\nare publicly available. In addition for the algorithm\nbeing efficient, we tried to minimize the computa-\ntion time. This work comply and adhere to ACL\ncode of ethics.2\nReferences\nChristoph Alt, Aleksandra Gabryszak, and Leonhard\nHennig. 2020. TACRED revisited: A thorough eval-\nuation of the TACRED relation extraction task. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 1558\u2013\n1569, Online. Association for Computational Linguis-\ntics.\nAnson Bastos, Abhishek Nadgeri, Kuldeep Singh, Isa-\niah Onando Mulang, Saeedeh Shekarpour, Johannes\nHoffart, and Manohar Kaul. 2021. Recon: Relation\nextraction using knowledge graph context in a graph\nneural network. In Proceedings of the Web Confer-\nence 2021 , WWW \u201921, page 1673\u20131685, New York,\nNY , USA. Association for Computing Machinery.\nAdrian Boteanu and Sonia Chernova. 2015. Solving\nand explaining analogy questions using semantic net-\nworks. Proceedings of the AAAI Conference on Arti-\nficial Intelligence , 29.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. CoRR , abs/1810.04805.\nMarkus Eberts and Adrian Ulges. 2019. Span-based\njoint entity and relation extraction with transformer\npre-training. CoRR , abs/1909.07755.\nSefika Efeoglu and Adrian Paschke. 2024. Retrieval-\naugmented generation-based relation extraction.\nDedre Gentner. 1983. Structure-mapping: A theoretical\nframework for analogy. Cognitive Science , 7(2):155\u2013\n170.\nDedre Gentner and Kenneth J. Kurtz. 2006. Relations,\nobjects, and the composition of analogies. Cognitive\nScience , 30(4):609\u2013642.\nDedre Gentner and Arthur B Markman. 1997. Structure\nmapping in analogy and similarity. Am. Psychol. ,\n52(1):45\u201356.\nDedre Gentner and Laura L Namy. 1999. Comparison\nin the development of categories. Cognitive Develop-\nment , 14(4):487\u2013513.\nFranz A. Heinsen. 2022. An algorithm for routing vec-\ntors in sequences.\n2https://www.aclweb.org/portal/content/\nacl-code-ethicsGeoffrey E. Hinton, Alex Krizhevsky, and Sida D. Wang.\n2011. Transforming auto-encoders. In Artificial Neu-\nral Networks and Machine Learning \u2013 ICANN 2011 ,\npages 44\u201351, Berlin, Heidelberg. Springer Berlin\nHeidelberg.\nKeith Holyoak. 2025. The Human Edge: Analogy and\nthe Roots of Creative Intelligence .\nPere-Llu\u00eds Huguet Cabot and Roberto Navigli. 2021.\nREBEL: Relation extraction by end-to-end language\ngeneration. In Findings of the Association for Com-\nputational Linguistics: EMNLP 2021 , pages 2370\u2013\n2381, Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nJohn E Hummel and Leonidas A A Doumas. 2023.\nAnalogy and similarity. In The Cambridge Hand-\nbook of Computational Cognitive Sciences , pages\n451\u2013473. Cambridge University Press.\nMingwei Li. Understanding capsule net-\nworks. http://hdc.cs.arizona.edu/~mwli/\nunderstanding-capsule-network/writing/ .\nAccessed: 2025-05-12.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining\napproach. CoRR , abs/1907.11692.\nJeffrey Loewenstein, Leigh Thompson, and Dedre Gen-\ntner. 2000. Analogical encoding facilitates knowl-\nedge transfer in negotiation. Psychonomic bulletin &\nreview , 6:586\u201397.\nAbhishek Nadgeri, Anson Bastos, Kuldeep Singh, Isa-\niah Onando Mulang\u2019, Johannes Hoffart, Saeedeh\nShekarpour, and Vijay Saraswat. 2021. KGPool: Dy-\nnamic knowledge graph context selection for relation\nextraction. In Findings of the Association for Com-\nputational Linguistics: ACL-IJCNLP 2021 , pages\n535\u2013548, Online. Association for Computational Lin-\nguistics.\nSeongsik Park and Harksoo Kim. 2021. Improving\nsentence-level relation extraction through curriculum\nlearning.\nSebastian Riedel, Limin Yao, and Andrew McCallum.\n2010. Modeling relations and their mentions without\nlabeled text. In ECML/PKDD .\nDan Roth and Wen-tau Yih. 2004. A linear program-\nming formulation for global inference in natural lan-\nguage tasks. In Proceedings of the Eighth Confer-\nence on Computational Natural Language Learn-\ning (CoNLL-2004) at HLT-NAACL 2004 , pages 1\u20138,\nBoston, Massachusetts, USA. Association for Com-\nputational Linguistics.\nSara Sabour, Nicholas Frosst, and Geoffrey E Hinton.\n2017. Dynamic routing between capsules.\n\n--- Page 11 ---\nLorenza Saitta and Jean-Daniel Zucker. 2013. Abstrac-\ntion in artificial intelligence and complex systems ,\n2013 edition. Springer, New York, NY .\nDaniil Sorokin and Iryna Gurevych. 2017. Context-\naware representations for knowledge base relation\nextraction. In Proceedings of the 2017 Conference on\nEmpirical Methods in Natural Language Processing ,\npages 1784\u20131789, Copenhagen, Denmark. Associa-\ntion for Computational Linguistics.\nGeorge Stoica, Emmanouil Antonios Platanios, and\nBarnab\u00e1s P\u00f3czos. 2021. Re-tacred: Addressing short-\ncomings of the tacred dataset.\nShikhar Vashishth, Rishabh Joshi, Sai Suman Prayaga,\nChiranjib Bhattacharyya, and Partha Talukdar. 2018.\nRESIDE: Improving distantly-supervised neural re-\nlation extraction using side information. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing , pages 1257\u20131266,\nBrussels, Belgium. Association for Computational\nLinguistics.\nYuhao Zhang, Victor Zhong, Danqi Chen, Gabor Angeli,\nand Christopher D. Manning. 2017. Position-aware\nattention and supervised data improve slot filling. In\nProceedings of the 2017 Conference on Empirical\nMethods in Natural Language Processing (EMNLP\n2017) , pages 35\u201345.\nWenxuan Zhou and Muhao Chen. 2022. An improved\nbaseline for sentence-level relation extraction. In Pro-\nceedings of the 2nd Conference of the Asia-Pacific\nChapter of the Association for Computational Lin-\nguistics and the 12th International Joint Conference\non Natural Language Processing (Volume 2: Short\nPapers) , pages 161\u2013168, Online only. Association for\nComputational Linguistics.\n\n--- Page 12 ---\nA Training and reproducibility\nFor all routing heads we use the code from (Hein-\nsen, 2022). Additionally, we use tokenizers from\nhttps://huggingface.co for bert-base-uncased, and\nroberta-large respectively. Our backbone models\nare too from https://huggingface.co. Furthermore,\nfor routing head H3, we train it on datasets Tacred,\nRetacred, and Tacredrev with batch size 64, learn-\ning rate 10\u22125, and on the dataset Wikidata with\nbatch size 128, and similar learning rate as for Ta-\ncred and its variants. For collection of experts we\nuse an smaller batch size of 24. For optimizer with\nuse Adam from torch.optim. Moreover, we find hid-\nden state of routing heads to have great influence\non performance. To that end, for H3, w used hid-\nden_d=256, and out_dimension=512. Moreover,\nwe trained the proposed model for Tacred and its\nvariants for 6 epochs, while we trained only for 1\nepoch on Wikidata.\nAnother point to note is: In case of wikidata,\nwhen entities did not have an entity type ( or in-\nstance of as in the dataset), we checked the Wiki-\ndata knowledge base to retrieve parent class as en-\ntity type3. Furthermore, when entities had several\nvalues as \"instance of\" or parent class, again we\nquery Wikidata to check if they have a common\nparent class, and used the parent class as the entity\ntype, if not, the most common class was uses for\nentity type. Lastly, unless explicitly mentioned,\nall experiments are done with Roberta-Large as\nbackbone.\nB Observation\nB.1 Are Sentential RE Datasets Truly\nSentential?\nTo answer if relation between the entities, can be\ninferred without reading the sentence, and only\nbe looking into entities, we trained and evaluated\nthe proposed model on entities with configuration\nas was discussed for the sentence. The result for\ndifferent configurations are recorded in the Table 4.\nAccordingly, most relation can be inferred without\nreading the concerned sentences.\nB.2 Noise in Wikidata\u2019s Labels\nOn examples which our model disagree with the\ndataset Wikidata, we found a pattern. specifically,\ngiven a pair (p0-p*), where p0 is label(\u2019no relation\u2019)\nprovided by the dataset, and p*(some relation other\n3https://query.wikidata.orgthan \"no relation\") predicted label, there is usu-\nally another category of predictions as (p*-p0). In\nboth group of examples the probability of p* being\ntrue is similar, regardless of label provided by the\ndataset. The group pairs, such as p0-p17 and p17-\np0; show confusion caused as a result of inaccurate\nlabels. Some examplese Table 6.\nLabel-Predictionprobability of\nP* being Truecount\nP0-P17 80.6 4090\nP0-P131 90.0 4037\nP0-P47 60.0 3518\nP0-P118 70.0 2155\nP0-P571 50.0 1718\nP0-All 70.0 29021\nP47-P0 60.0 12184\nP131-P0 80.0 4775\nP17-P0 70.0 4312\nP361-P0 60.0 2152\nP463-P0 70.0 1546\nAll-P0 60.0 40155\nlabel!=prediction - 106534\nSome Codes with Labels\nP131:located in the administrative territorial entity\nP17:country\nP47:shares border with\nP118:league\nP571:inception\nP47:shares border with\nP361:part of\nP463:member of\nTable 6: Top categories(sorted) on which model\u2019s pre-\ndictions does not match with the label from benchmark.\n* represent a relation other than \u2019no relation\u2019. The proba-\nbility here is calculated by sampling 10 random example\nfrom each category, and then manually checking if p*\nholds.\n\n--- Page 13 ---\nLabel-Prediction Sample\nP0-P17\nP0:No relation\nP17:countryLos Dominicos is a metro station on\nLine 1 of the Santiago Metro in San-\ntiago , Chile, and is also the eastern\nterminal of this line .\nP0-P131\nP0:No relation\nP131: Located in the\nadministrative territorial entityBoechout is a railway station in Boe-\nchout , Antwerp , Belgium .\nP0-P47\nP0:No relation\nP47:Shares border withThere are now approximately twenty\nrestaurants in operation in Georgia ,\nand about nine more in North Carolina\n, South Carolina , Florida , and Ten-\nnessee.\nTable 7: Random sample from P0\u2212P\u2217, where p\u2217is any relation from relation set other than no relation, and p0 is\nno relation\nLabel-Prediction Sample\nP6-P138\nP6:head of government\nP138:named afterShe later served in the Blair ministry\nunder Prime Minister Tony Blair in a\nnumber of roles , becoming Britains\nfirst female Foreign Secretary in 2006\n.\nP264-P136\nP264:record label\nP136:GenreCD1 is the unofficial name of an unti-\ntled album by English industrial music\nband Throbbing Gristle , released in\n1986 through record label Mute .\nP1416-P102\nP1416:affiliation\nP102:member of political partyOther famous Solidarity activists such\nas [e11] Anna Walentynowicz Soli-\ndarity activists such as Anna Walen-\ntynowicz , Zbigniew Romaszewski and\nAntoni Macierewicz have visited the\nBasilica as well .\nTable 8: Random sample from p\u2217\u2212p\u2217, where p\u2217is any relation from relation set other than no relation. Consider\nthe first row in which both label and prediction is correct.",
  "project_dir": "artifacts/projects/enhanced_cs.CL_2508.21049v1_Re_Representation_in_Sentential_Relation_Extractio",
  "communication_dir": "artifacts/projects/enhanced_cs.CL_2508.21049v1_Re_Representation_in_Sentential_Relation_Extractio/.agent_comm",
  "assigned_at": "2025-08-29T20:56:39.402799",
  "status": "assigned"
}